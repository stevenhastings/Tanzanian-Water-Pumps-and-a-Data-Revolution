# -*- coding: utf-8 -*-
"""Tanzanian_WaterPump_Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pddRT7KurVV5KE62W-ES_f8C3jVA7_Bn

# TANZANIAN WATER PUMP CHALLENGE:
1. pip install libraries 
2. import necessary packages
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install category_encoders==2.*
# !pip install pandas-profiling==2.*
# !pip install pandas-profiling[notebook]
# !pip install geopandas

import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly
import plotly.express as px
import geopandas
from sklearn.model_selection import train_test_split
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

"""## Import Data Sets from [DrivenData Competition Site](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/data/)"""

train_values = pd.read_csv("/content/drive/MyDrive/Training Set Values.csv")
train_labels = pd.read_csv('/content/drive/MyDrive/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv')

train = pd.merge(train_values, train_labels)

test = pd.read_csv('/content/drive/MyDrive/702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv')

submission_format = pd.read_csv('/content/drive/MyDrive/SubmissionFormat.csv')

# split data into training and validation sets
train, val = train_test_split(train, train_size=0.80, test_size=0.20,
                              stratify=train['status_group'], random_state=42)

print(f"training shape: {train.shape}\n validation shape: {val.shape}\n testing shape: {test.shape}")

"""### Multi-class classification uses majority class for its naive baseline measure"""

classes = train['status_group'].value_counts(normalize=True)

majority_class = classes[0]
print(classes)
print(f"\n\nMajority Class = Functional = {majority_class}")
print("\n If we predicted the water pump to be functional every time we would be correct around 54% of the time")

"""## Sparse Plot to give shape to our Data"""

px.scatter(train, x='longitude', y='latitude', color='status_group', opacity=0.1)

train[['longitude', 'latitude']].describe()

"""## Data Rancher!!!"""

def wrangle(X):
    """ Wrangle train, validate, and test sets in the same way. """

    # Prevent SettingWithCopyWarning
    X = X.copy()

    # let's set the index to be our id column
    # X.set_index('id', inplace=True)
    # About 3% of the time, latitude has small values near zero,
    # outside of Tanzania, so we'll treat these values like zero.
    X['latitude'] = X['latitude'].replace(-2e-08, 0)

    # for now let's replace the zeros in the data that are likely NaN values with Nulls
    # we'll impute the missing values later.
    cols_with_zeros = ['longitude', 'latitude']
    for col in cols_with_zeros:
        X[col] = X[col].replace(0, np.nan)

    # quantity and quantity_group are duplicate columns so we can drop one
    # X = X.drop(columns='quantity_group')
    # return wrangled dataframe
    return X 

train = wrangle(train)
val = wrangle(val)
test = wrangle(test)

train.drop(columns=['waterpoint_type_group', 'extraction_type_class', 'extraction_type_group','quantity_group', 'source_type', 'quality_group','wpt_name','subvillage', 'region','lga','ward','num_private','district_code','scheme_name'], inplace=True)

gdf = geopandas.GeoDataFrame(train, geometry=geopandas.points_from_xy(train.longitude, train.latitude))

gdf.head()

"""### The Plotly graph above is nice but we can elaborate"""

fig = px.scatter_mapbox(train, lat='latitude', lon='longitude', color='status_group', opacity=0.1) #width=2000, height=4000
fig.update_layout(mapbox_style='stamen-terrain')
fig.show()

train['public_meeting'] = train['public_meeting']*1

train.set_index('id', inplace=True)

print(train.columns)
train.head()

"""## Data Munging Break Down"""

# the status_group column is the target
target = 'status_group'

# get the dataframe with all train columns except the target and id
train_features = train.drop(columns=target)

# get a list of the numeric features
numeric_features = train_features.select_dtypes(include='number').columns.tolist()

# get a series with the cardinality of the non-numeric features
cardinality = train_features.select_dtypes(exclude='number').nunique()

# get a list of all categorical features with cardinality <= 50
categorical_features = cardinality[cardinality <= 50].index.tolist()

# combine the lists
features = numeric_features + categorical_features
print(features)

"""## Train/Validate/Test Split"""

X_train = train[features]
y_train = train[target]
X_val = val[features]
y_val = val[target]
X_test = test[features]

"""## Logistic Regression the Hard Way"""

# import packages and libraries
import category_encoders as ce
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# Instantiation
encoder = ce.OneHotEncoder(use_cat_names=True)
imputer = SimpleImputer(strategy='mean')
scaler = StandardScaler()
model = LogisticRegression(max_iter=1000)

# fitting and transforming training
X_train_encoded = encoder.fit_transform(X_train)
X_train_imputed = imputer.fit_transform(X_train_encoded)
X_train_scaled = scaler.fit_transform(X_train_imputed)
model.fit(X_train_scaled, y_train)

# fitting and transforming validation
X_val_encoded = encoder.transform(X_val)
X_val_imputed = imputer.transform(X_val_encoded)
X_val_scaled = scaler.transform(X_val_imputed)
#accuracy score
print('Validation Accuracy', model.score(X_val_scaled, y_val))

# fitting and transforming test set
X_test_encoded = encoder.transform(X_test)
X_test_imputed = imputer.transform(X_test_encoded)
X_test_scaled = scaler.transform(X_test_imputed)

y_pred = model.predict(X_test_scaled)

"""## Pipelines == Less Code"""

pipeline = make_pipeline(
    encoder,
    imputer,
    scaler,
    model
)

# fit on train
pipeline.fit(X_train, y_train)

# score on validation
print(f"Validation Accuracy: {pipeline.score(X_val, y_val)}")

# Predict on Test set
y_pred = pipeline.predict(X_test)

"""## Get  Coefficients"""

pipeline.named_steps

pipeline.named_steps['logisticregression'].coef_

"""## Plot Coefficients"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

model = pipeline.named_steps['logisticregression']
encoder = pipeline.named_steps['onehotencoder']
encoded_columns = encoder.transform(X_val).columns
coefficients = pd.Series(model.coef_[0], encoded_columns)
# plt.figure(figsize=(10,30))
coefficients.sort_values().tail(10).plot.barh(color='blue');

"""### Least Important"""

coefficients.sort_values().head(10).plot.barh(color='blue');

"""### We get the coefficients in order to understand our model.

If all we were interested in was the accuracy we could bypass this step but I find that it gives you a deeper understanding of what's going on rather than just taken the number it shoots out and running with it.  Plus using the coefficient importances gives you insight if you wanted to tighten up the dataset without removing important features to improve the model accuracy.

# Now with Decision Trees
"""

from sklearn.tree import DecisionTreeClassifier

pipeline = make_pipeline(
    ce.OneHotEncoder(use_cat_names=True),
    SimpleImputer(strategy='mean'),
    DecisionTreeClassifier(random_state=42)
)

# fit on train
pipeline.fit(X_train, y_train)

# score on validation
print('Train Accuracy', pipeline.score(X_train, y_train))
print(f"Validation Accuracy: {pipeline.score(X_val, y_val)}")

# Predict on Test set
y_pred = pipeline.predict(X_test) # val accuracy is a bit better here than before

"""# Visualize our Tree Model"""

import graphviz
from sklearn.tree import export_graphviz

model = pipeline.named_steps['decisiontreeclassifier']
encoder = pipeline.named_steps['onehotencoder']
encoded_columns = encoder.transform(X_val).columns

dot_data = export_graphviz(model,
                           out_file=None,
                           max_depth=3,
                           feature_names=encoded_columns,
                           class_names=model.classes_,
                           impurity=False,
                           filled=True,
                           proportion=True,
                           rounded=True)

display(graphviz.Source(dot_data))

"""*   training accuracy is really high
*   indicating model maybe overfitting
*   let's try reducing model's complexity
*   by changing the min_samples_leaf parameter
"""

pipeline = make_pipeline(
    ce.OneHotEncoder(use_cat_names=True),
    SimpleImputer(strategy='mean'),
    DecisionTreeClassifier(min_samples_leaf=20, random_state=42)
)

pipeline.fit(X_train, y_train)

# Score on train and val
print("Train Accuracy: ", pipeline.score(X_train, y_train))
print(f"Validation Accuracy: {pipeline.score(X_val, y_val)}")

"""# Feature Importances
*   Sometimes you need to bias your model against the training set in order to improve its generalizability.  
"""

### FEATURE IMPORTANCES: the coefficients for tree based models
model = pipeline.named_steps['decisiontreeclassifier']

encoder = pipeline.named_steps['onehotencoder']
encoded_columns = encoder.transform(X_val).columns

feature_importances = pd.Series(model.feature_importances_, encoded_columns)
# plt.figure(figsize=(10,30))
feature_importances.sort_values().tail(10).plot.barh()

"""## Comparing a Logistic Regression with 2 non-linear features, longitude and latitude"""

train_location = X_train[['longitude', 'latitude']].copy()
val_location = X_val[['longitude', 'latitude']].copy()

# with just longitude and latitude a Logistic Regression can't beat the majority class baseline

lr = make_pipeline(
    SimpleImputer(),
    LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1)
)

lr.fit(train_location, y_train)
print("Logistic Regression: ")
print("Train Accuracy: ", lr.score(train_location, y_train))
print("Validation Accuracy: ", lr.score(val_location, y_val))

"""## Now the same comparison with DecisionTreeClassifier"""

dt = make_pipeline(
    SimpleImputer(),
    DecisionTreeClassifier(max_depth=16, random_state=42)
)

dt.fit(train_location, y_train)
print('Decision Tree: ')
print(f"Train Accuracy: {dt.score(train_location, y_train)}")
print(f"Validation Accuracy: {dt.score(val_location, y_val)}")

"""## Visualizing Logistic Regression Predictions"""

import itertools
from math import floor
import seaborn as sns

def pred_heatmap(model, X, features, class_index=-1, title='', num=100):

    """
    Visualize predicted probabilities, for classifier fit on 2 numeric features

    Parameters
    ----------
    model : scikit-learn classifier, already fit
    X : pandas dataframe, which was used to fit the model
    features : list of strings, column names of the 2 numeric features
    class_index : integer, index of class label
    title : string, title of plot
    num : integer, number of grid points for each feature
    """
    feature1, feature2 = features
    min1, max1 = X[feature1].min(), X[feature1].max()
    min2, max2 = X[feature2].min(), X[feature2].max()
    x1 = np.linspace(min1, max1, num)
    x2 = np.linspace(max2, min2, num)
    combos = list(itertools.product(x1, x2))
    y_pred_proba = model.predict_proba(combos)[:, class_index]
    pred_grid = y_pred_proba.reshape(num, num).T
    table = pd.DataFrame(pred_grid, columns=x1, index=x2)
    plot_every_n_ticks = int(floor(num/4))
    sns.heatmap(table, #vmin=0, vmax=1,
                xticklabels=plot_every_n_ticks,
                yticklabels=plot_every_n_ticks)
    plt.xlabel(feature1)
    plt.ylabel(feature2)
    plt.title(title)
    plt.show()

pred_heatmap(lr, train_location, 
             features=['longitude', 'latitude'],
             class_index=0, 
             title='Logistic Regression, Predicted Probability, "functional"')

pd.Series(lr.named_steps['logisticregression'].coef_[0],
          train_location.columns)

"""## Visualize Decision Tree Predictions using function"""

pred_heatmap(dt, 
             train_location, 
             features=['longitude', 'latitude'],
             class_index=0, 
             title='Decision Tree, predicted probability, "functional"')

# deeper view into how a tree grows, branch by branch

from IPython.display import display, HTML

for max_depth in range(4, 21):

    # Fit the decision tree
    dt = make_pipeline(
        SimpleImputer(),
        DecisionTreeClassifier(max_depth=max_depth, random_state=42)
    )

    dt.fit(train_location, y_train)

    # Display depth and scores
    display(HTML(f'Max Depth {max_depth}'))
    display(HTML(f'Train Accuracy {dt.score(train_location, y_train):.2f}'))
    display(HTML(f'Validation Accuracy {dt.score(val_location, y_val):.2f}'))

    # Plot heatmap of predicted probabilities
    pred_heatmap(dt, train_location, features=['longitude', 'latitude'],
                 class_index=0, title='Predicted Probability, "functional"')
    
    # Plot tree
    dot_data = export_graphviz(dt.named_steps['decisiontreeclassifier'],
                               out_file=None,
                               max_depth=2,
                               impurity=False,
                               filled=True,
                               proportion=True,
                               rounded=True)
    
    display(graphviz.Source(dot_data))

"""# SECTION II


---



---



---



---



---


"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import sys
# !pip install eli5
# from sklearn.ensemble import RandomForestClassifier

pd.set_option('display.max_rows', None)

"""### Redefining the Wrangle Function"""

def wrangle(X):
    """ Wrangle train, validate, and test sets in the same way. """

    # Prevent SettingWithCopyWarning
    X = X.copy()

    # let's set the index to be our id column
    # X.set_index('id', inplace=True)
    # About 3% of the time, latitude has small values near zero,
    # outside of Tanzania, so we'll treat these values like zero.
    X['latitude'] = X['latitude'].replace(-2e-08, 0)

    # for now let's replace the zeros in the data that are likely NaN values with Nulls
    # we'll impute the missing values later.
    cols_with_zeros = ['longitude', 'latitude', 'construction_year',
                       'gps_height', 'population']
    for col in cols_with_zeros:
        X[col] = X[col].replace(0, np.nan)
        X[col+'_MISSING'] = X[col].isnull()

    # Convert date recorded to datetime
    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)

    # Extract components from date_recorded, then drop the original column
    X['year_recorded'] = X['date_recorded'].dt.year
    X['month_recorded'] = X['date_recorded'].dt.month
    X['day_recorded'] = X['date_recorded'].dt.day
    X = X.drop(columns='date_recorded')

    # Feature Engineering: How many years from construction_year to date_recorded?
    X['age'] = X['year_recorded'] - X['construction_year']
    X['years_MISSING'] = X['age'].isnull().sum().sum()

    # return wrangled dataframe
    return X 

train = wrangle(train)
val = wrangle(val)
test = wrangle(test)

"""# Splitting the Data"""

target = 'status_group'
X_train = train.drop(columns=target)
y_train = train[target]
X_val = val.drop(columns=target)
y_val = val[target]
X_test = test

"""## Cleaning Shop Column Drop"""

X_train.drop(columns="geometry", inplace=True)

X_val.drop(columns=[ 'district_code','lga', 'num_private', 'quality_group', 'region', 
                    'wpt_name','id', 'waterpoint_type_group', 'source_type', 'ward', 'scheme_name',
                    'quantity_group', 'extraction_type_class', 'extraction_type_group',
                    'subvillage'], inplace=True)

X_test.drop(columns=['id', 'lga', 'num_private', 'quality_group', 'region', 'district_code',
                     'waterpoint_type_group', 'source_type', 'wpt_name', 'ward', 'scheme_name', 
                     'quantity_group', 'extraction_type_class', 'extraction_type_group',
                     'subvillage'], inplace=True)

"""## Random Forests: Ensemble of Decision Trees"""

pipeline = make_pipeline(
    ce.OrdinalEncoder(),
    SimpleImputer(), #default is 'mean'
    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
)

# fit on train, score on val
pipeline.fit(X_train, y_train)
print('validation accuracy: ', pipeline.score(X_val, y_val))

# remember baseline for multi-class classification. . . 
y_train.value_counts(normalize=True).max() # majority class

"""# Permutation Importances


---



---



---

Permutation importance is a good compromise between Feature Importance based on impurity reduction ( which is fastest ) and Drop Column Importance ( which is best )

Importance can be measured by looking at how much the score (accuracy, F1, R^2, etc. . . any score we're interested in) decreases when a feature is not available.

To do that one can remove feature from the dataset, re-train the estimator and check the score. But, it requires re-training an estimator for each feature, which can be computationally intensive. 

To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn't work as-is, because estimators expect features to be present. So, instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the same distribution as original feature values ( as otherwise estimator may fail ). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples' feature values - this is how permutation importance is computed.

The method is most suitable for computing importances when a number of columns features are not large; it can become resource-intensive otherwise.
"""

import eli5
from eli5.sklearn import PermutationImportance

transformers = make_pipeline(
    ce.OrdinalEncoder(),
    SimpleImputer(strategy='median')
)

X_train_transformed = transformers.fit_transform(X_train)
X_val_transformed = transformers.transform(X_val)

model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
model.fit(X_train_transformed, y_train)

permuter = PermutationImportance(
    model,
    scoring='accuracy',
    n_iter=5, # run each permutation 5 times and take the average
    random_state=42
)

permuter.fit(X_val_transformed, y_val)

feature_names = X_val.columns.tolist()

pd.Series(permuter.feature_importances_, feature_names).sort_values(ascending=False)

"""### eli5 Weights"""

eli5.show_weights(
    permuter,
    top=None,
    feature_names=feature_names
)

print('shape before removing features: ', X_train.shape)

"""### New Mask Using Feature Importances"""

minimum_importance = 0

mask = permuter.feature_importances_ > minimum_importance

features = X_train.columns[mask]
X_train_selected = X_train[features]

X_train_selected.shape

X_val_selected = X_val[features]

pipeline = make_pipeline(
    ce.OrdinalEncoder(),
    SimpleImputer(strategy='median'),
    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
)

# fit on train, score on val
pipeline.fit(X_train_selected, y_train)
print("Validation Accuracy: ", pipeline.score(X_val_selected, y_val))

"""### Dropping the 5 negative features is a pretty clear benefit
*   remove more features based on standard error???
"""

new_mask = permuter.feature_importances_ - permuter.feature_importances_std_ > 0

features = X_train.columns[new_mask]

X_train_selected = X_train[features]

X_train_selected.shape

"""## RandomForestClassifier Pipeline and Metrics"""

X_val_selected = X_val[features]

pipeline = make_pipeline(
    ce.OrdinalEncoder(),
    SimpleImputer(strategy='median'),
    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
)

# fit on train, score on val
pipeline.fit(X_train_selected, y_train)
print("Validation Accuracy: ", pipeline.score(X_val_selected, y_val))

"""# XGBoost for gradient boosting"""

from xgboost import XGBClassifier

pipeline = make_pipeline(
    ce.OrdinalEncoder(),
    XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1)
)

pipeline.fit(X_train, y_train)

"""## Metrics"""

from sklearn.metrics import accuracy_score
y_pred = pipeline.predict(X_val)
print('validation accuracy: ', accuracy_score(y_val, y_pred))

"""# Early Stopping
## Without a Pipeline
"""

# stopping the use of a pipeline approach in order to try out different parameters
encoder = ce.OrdinalEncoder()
X_train_encoded = encoder.fit_transform(X_train)
X_val_encoded = encoder.transform(X_val)

model = XGBClassifier(
    n_estimators=100, #Number of estimators(if many) will depend on early stopping
    max_depth=7, # Deeper trees to help with high cardinality features
    learning_rate=0.5, # Trying out a higher learning rate
    n_jobs=-1
)

eval_set = [(X_train_encoded, y_train),
            (X_val_encoded, y_val)]

model.fit(X_train_encoded, y_train,
          eval_set=eval_set,
          eval_metric='merror',
          early_stopping_rounds=50) # Stop if the score hasn't improved in 50 rounds

"""## Visualize Validation Curve for XGBoost model"""

results = model.evals_result()

train_error = results['validation_0']['merror']
val_error = results['validation_1']['merror']

epoch = list(range(1, len(train_error)+1))

plt.plot(epoch, train_error, label='Train')
plt.plot(epoch, val_error, label='Validation')
plt.ylabel('Classification Error')
plt.xlabel('Model Complexity (n_estimators)')
plt.title('Validation Curve for this XGBoost model')
plt.legend();

"""## Accuracy would be (1 - bestValidationError)"""

# accuracy would be 1 - our best validation error
print(1 - 0.200673)
print("what a let down :(")

"""# DONE ! ! !

---



---



---



---


"""